\documentclass[12pt, a4paper]{report}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage{amsmath,amssymb,amsthm,mathtools,cases,bm}

\theoremstyle{definition}
\newtheorem{Example}{Example}
\newtheorem{Theorem}{Theorem}
\newtheorem{ToDo}{ToDo}

\usepackage{varioref,hyperref,cleveref}
\usepackage[pdftex]{graphicx}
\usepackage{caption,float,epstopdf,subfig}

\renewcommand{\thefigure}{\arabic{figure}}

\usepackage{listings}
\usepackage[dvipsnames]{xcolor}

\lstset{language=[ISO]C++,
				backgroundcolor = \color{blue!5!white},
				aboveskip=10pt, belowskip=10pt,
				numbers=left, numberstyle=\tiny,
				tabsize=2,
				breaklines=true,
				basicstyle=\small\ttfamily,
				keywordstyle = \color{blue!50!cyan}\bfseries,
				commentstyle = \color{darkgray!90},
				stringstyle = \color{OliveGreen},
				morecomment=[l][\color{violet!90!black}]{\#},
				identifierstyle=\color{blue!25!black}
				literate=
				{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
				{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
				{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
				{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}} 
	
\title{\textbf{Deep Learning for PDEs}\\
			\large Report of the joint APSC-NAPDE courses project}
\author{Paolo Joseph Baioni\footnote{\href{mailto:paolojoseph.baioni@mail.polimi.it}{paolojoseph.baioni@mail.polimi.it}}}
\date{\today}

\begin{document}

	
\maketitle

\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	

\chapter*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
\tableofcontents
\clearpage
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The numerical solution of partial derivative equations (PDEs) plays a fundamental role in applied mathematics, science and engineering. \\
The recent advances in machine learning (ML) and the successes obtained by the application of these techniques in various areas suggest the possibility of using ML in solving PDEs. This approach has considerable potential compared to other numerical methods: from the possibility of writing mesh-free algorithms to that of solving data-learned equations, whose explicit functional form is unknown; however the theory is not yet developed and consequently important results of convergence and stability are missing, as well as general rules that would allow to identify the optimal parameters for the design of numerical codes. Finally, some intrinsic characteristics of the method make it considerable as a  tool which is complementary to traditional numerical methods, finding application in the field of real-time control. \\
The aim of this project is to get into deep learning techniques and study their  possible applications to PDEs, also reporting some useful theoretical results, as a complement to the methods illustrated during the NAPDE course, and to implement from scratch a Numerical Analysis relevant Neural Networks based C++ program, so to both gain and verify a low-level, detailed and complete understanding of the method and to experiment on some of the programming techniques that have been deepened during the APSC and \textit{``Strumenti di sviluppo e distribuzione di software per la ricerca scientifica''} courses held at PoliMi.\\
The structure of the report is as follows.\\
In chapter \ref{chapter1} we present the general architecture of a Deep Neural Network (DNN) and the main idea of functioning of the algorithm that allows it to learn from the data (Deep Learning), consisting of two phases, called \textit{forward propagation} and \textit{backward propagation}. Therefore, some  sector-specific issues are considered in detail, such as: distinction between train, development and test sets, problems related to overfitting, possible regularization techniques aimed at reducing it, different optimization algorithms and an overview of the main parameters that must be tuned adequately to get good results.\\
In chapter \ref{chapter2} two possible approaches to solving PDEs via DNNs are exposed, also highlighting some choices that can be made in the formulation of the problem and in treating the boundary conditions. We then deepen the comparison between DNNs and the piecewise continuous linear function which are used as bases of finite element spaces of order one, in order to provide a greater intuition of the reasons why the DNN-based method works and to identify, albeit in this particular case, some general indications on the ideal number of nodes and layers of the DNN.\\
In chapter \ref{chapter3} we explain the programming architectural choices and the structure of the developed code, freely available on {GitHub\footnote{\href{https://github.com/pjbaioni/neural-net}{\emph{https://github.com/pjbaioni/neural-net}}}, as well as possible extensions.\\
In chapter \ref{chapter4}, after providing instructions on how to compile, link and run the program, we show some examples by means of benchmark cases.\\
In appendix we report the \cite{freefem++} code written for the computations performed in section \ref{section2.2}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Neural Networks and Deep Learning}\label{chapter1}

\section{Architecture and operation of a Deep Neural Network}\label{section1.1}

\section{Further insights}\label{section1.2}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Application to PDEs}\label{chapter2}

\section{The Poisson-Dirichlet problem}\label{section2.1}

\section{Some more in-depth theoretical results}\label{section2.2}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Implementation of neural-net}\label{chapter3}
In this chapter an in-depth view of a Deep Learning C++ program, called \textit{neural-net}, is given.\\
The program has been built using the \cite{git} distributed version control system, with GitHub as hosting service, and can be freely cloned or downloaded from the author's page: \href{https://github.com/pjbaioni/neural-net}{\emph{https://github.com/pjbaioni/neural-net}}.\\
In order to build and run the program, some not included dependencies are needed, in particular a C++ compiler, \cite{make}, \cite{gnuplot}, \cite{eigen} and \cite{boost} libraries, while to build the documentation a TeX compiler has to be used; more details are given in the README.md file and in chapter \ref{chapter4}, where it is shown how to build the program and run an example. 

\section{Directory tree}
Downloading the repository ``neural-net'' one's find:
\begin{itemize}
	\item the \textit{data} folder, which contains the input and output data in .dat and .pot (for \cite{getpot}) format, \cite{gnuplot} scripts in .gnu format, and their graphical output in .png format;
	\item the \textit{doc} folder, containing this documentation in .tex and .pdf format, plus the \textit{img} sub-folder where there are the images included by the TeX file;
	\item the \textit{include} folder, containing the headers \textit{GetPot.hpp}, an utility used for parsing parameters file and command line option, \textit{gnuplot-iostream.hpp}, an utility used to output a graphical representation of the results in an interactive way, \textit{NeuralNetwork.hpp}, which holds the NeuralNetwork \textbf{class}, and \textit{Optimizers.hpp}, where all the optimizers employable from NeuralNetwork are defined as \textbf{class templates};
	\item the \textit{src} folder, which contain the source files main.cpp, NeuralNetwork.cpp, where the NeuralNetwork class member functions are defined, the Makefile which can be used to automatic build the main program, and the \textit{write\_set} sub-folder, where the \textit{write\_set.cpp} file used to generate datasets is placed;
	\item the COPYING file, a plain text file containing copying informations and licenses;
	\item the README.md file, a markdown file containing basic informations;
	\item the (hidden) \textit{.gitignore} file, that trace the file extensions which are not being pushed to the remote, mainly compilation files, executables and comments file.
\end{itemize}
\noindent Despite the different extensions, every non-png nor non-pdf file can be opened by any text editor.
\section{The Neural Network class}
\section{The Optimizers class templates}
\section{Write\_set and data}
\section{The main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Examples}\label{chapter4}

\section{Building instructions}
\section{Reconstruction of a wave packet}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Conclusions and further developments}
\addcontentsline{toc}{chapter}{Conclusions and further developments}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
\chapter*{Appendix}\label{appendix}
\addcontentsline{toc}{chapter}{Appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\begin{thebibliography}{10}
	\addcontentsline{toc}{chapter}{Bibliography}
	
	\bibitem[APSC]{PACS} Advanced Programming for Scientific Computing course lectures, Politecnico di Milano, 2019.
	\bibitem[Andrew NG]{AndrewNG} Professor Andrew NG's Deep Learning MOOC at Coursera, \href{https://www.coursera.org/specializations/deep-learning}{\emph{https://www.coursera.org/specializations/deep-learning}}, consulted in 2019.
	\bibitem[Boost]{boost} Boost C++ libraries, \href{https://www.boost.org/}{\emph{https://www.boost.org/}}
	\bibitem[cppreference.com]{cppreference} An online reference of the C++ language, \href{https://en.cppreference.com/w/cpp}{\emph{https://en.cppreference.com/w/cpp}}
	\bibitem[Eigen]{eigen} Eigen C++ template library for linear algebra, \href{https://eigen.tuxfamily.org/}{\emph{https://eigen.tuxfamily.org/}}
	\bibitem[FreeFem++]{freefem++} FreeFem++ PDE solver, \href{https://freefem.org/}{\emph{https://freefem.org/}}
	\bibitem[GetPot]{getpot} GetPot command line parser, \href{http://getpot.sourceforge.net/}{\emph{http://getpot.sourceforge.net/}}
	\bibitem[Git]{git} Git distributed version control system, \href{https://git-scm.com/}{\emph{https://git-scm.com/}}
	\bibitem[gnuplot]{gnuplot} A GNU graphing utility, \href{http://www.gnuplot.info/}{\emph{http://www.gnuplot.info/}}
	\bibitem[gnuplot-iostream]{gnuplot-iostream} A C++ interface to gnuplot, \href{https://github.com/dstahlke/gnuplot-iostream}{\emph{https://github.com/dstahlke/gnuplot-iostream}}
	\bibitem[Jinchao Xu et al.]{Jinchao} Juncai He, Lin li, Jinchao Xu, Chunyue Zheng, \emph{ReLU Deep Neural Networks and Linear Finite Elements}, 2018, found at \href{https://arxiv.org/abs/1807.03973}{\emph{https://arxiv.org/abs/1807.03973}}
	\bibitem[Kailai et al.]{Kailai} Kailai Xu, Bella Shi, Shuyi Yin, code and technical report of the project for the course CS230 \emph{Deep Learning, Winter 2018, Stanford University}, found at \href{https://github.com/kailaix/nnpde}{\emph{https://github.com/kailaix/nnpde}}
	\bibitem[Kingma-LeiBa]{Kingma} Diederik P. Kingma, Jimmy Lei Ba,\emph{Adam: a method for stochastic optimization}, 2015, found at
	\href{https://arxiv.org/abs/1412.6980}{\emph{https://arxiv.org/abs/1412.6980}} 
	\bibitem[Make]{make} GNU Make doc at \href{https://www.gnu.org/software/make/manual}{\emph{https://www.gnu.org/software/make/manual}}
	\bibitem[Weinan-Bing]{Weinan} Weinan E, Bing Yu \emph{The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems}, 2017, found at \href{https://arxiv.org/abs/1710.00211}{\emph{https://arxiv.org/abs/1710.00211}}
	
\end{thebibliography}


\end{document}	
