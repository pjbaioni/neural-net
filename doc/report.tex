\documentclass[12pt, a4paper]{report}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{setspace,enumerate}
\usepackage{amsmath,amssymb,amsthm,mathtools,cases,bm}

\theoremstyle{definition}
\newtheorem{Example}{Example}
\newtheorem{Theorem}{Theorem}
\newtheorem{ToDo}{ToDo}

\usepackage{varioref,hyperref,cleveref}
\usepackage[pdftex]{graphicx}
\usepackage{caption,float,epstopdf,subfig}

\renewcommand{\thefigure}{\arabic{figure}}

\usepackage{listings}
\usepackage[dvipsnames]{xcolor}

\lstset{
	language=[ISO]C++,
	backgroundcolor = \color{blue!5!white},
	aboveskip=10pt, belowskip=10pt,
	numbers=left, numberstyle=\tiny,
	tabsize=2,
	breaklines=true,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle = \color{blue!50!cyan}\bfseries,
	commentstyle = \color{darkgray!90},
	stringstyle = \color{OliveGreen},
	morecomment=[l][\color{violet!90!black}]{\#},
	identifierstyle=\color{blue!25!black}
}


\lstdefinelanguage{FreeFem++}[]{C++}{
	morekeywords={int, border, t, pi, label, x, y, sin, cos, pi, mesh, buildmesh, fespace, P1, cout, endl, real, P2, plot, fill, value, cmm, wait, func, macro, solve, solver, CG, int2d, sqrt}
	basicstyle=\footnotesize\ttfamily,
	keywordstyle = \color{blue!25!black},
	stringstyle = \color{blue!25!black}
}
	

\renewcommand*{\thefootnote}{\fnsymbol{footnote}} 
	
\title{\textbf{Deep Learning for PDEs}\\
			\large Report of the joint APSC-NAPDE courses project}
\author{Paolo Joseph Baioni\footnote{\href{mailto:paolojoseph.baioni@mail.polimi.it}{paolojoseph.baioni@mail.polimi.it}}}
\date{\today}

\begin{document}

	
\maketitle

\renewcommand*{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	

%\chapter*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
\tableofcontents
\clearpage
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The numerical solution of partial derivative equations (PDEs) plays a fundamental role in applied mathematics, science and engineering. \\
The recent advances in machine learning (ML) and the successes obtained by the application of these techniques in various areas suggest the possibility of using ML in solving PDEs. This approach has considerable potential compared to other numerical methods, for example it makes feasible to write mesh-free algorithms, however the theory is not yet developed and consequently important results of convergence and stability are missing, as well as general rules that would allow to identify the optimal parameters for the design of numerical codes. Finally, some intrinsic characteristics of the method make it considerable as a  tool which is complementary to traditional numerical methods, finding application in the field of real-time control. \\
The aim of this project is to get into deep learning techniques and study their  possible applications to PDEs, also reporting some useful theoretical results, as a complement to the methods illustrated during the NAPDE course, and to implement from scratch a Numerical Analysis relevant Neural Networks based C++ program, so to both gain and verify a low-level, detailed and complete understanding of the method and to experiment on some of the programming techniques that have been deepened during the APSC and \textit{``Strumenti di sviluppo e distribuzione di software per la ricerca scientifica''} courses held at PoliMi.\\
The structure of the report is as follows.\\
In chapter \ref{chapter1} we present the general architecture of a Deep Neural Network (DNN) and the main idea of functioning of the algorithm that allows it to learn from the data (Deep Learning), consisting of two phases, called \textit{forward propagation} and \textit{backward propagation}. Therefore, some  sector-specific issues are considered in detail, such as: distinction between train, development and test sets, problems related to overfitting, possible regularization techniques aimed at reducing it, different optimization algorithms and an overview of the main parameters that must be tuned adequately to get good results.\\
In chapter \ref{chapter2} two possible approaches to solving PDEs via DNNs are exposed, also highlighting some choices that can be made in the formulation of the problem and in treating the boundary conditions. We then deepen the comparison between DNNs and the piecewise continuous linear function which are used as bases of finite element spaces of order one, in order to provide a greater intuition of the reasons why the DNN-based method works and to identify, albeit in this particular case, some general indications on the ideal number of nodes and layers of the DNN.\\
In chapter \ref{chapter3} we explain the programming architectural choices and the structure of the developed code, freely available on {GitHub\footnote{\href{https://github.com/pjbaioni/neural-net}{\emph{https://github.com/pjbaioni/neural-net}}}, as well as possible extensions.\\
In chapter \ref{chapter4}, after providing instructions on how to compile, link and run the program, we show some examples by means of benchmark cases.\\
In \hyperref[appendix]{appendix} we report the \cite{freefem++} code written for the computations performed in section \ref{section2.2}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Neural Networks and Deep Learning}\label{chapter1}
In this chapter we give an introduction to an emerging branch of artificial intelligence which is called \textit{Deep Learning}, and we focus in particular to how to build a \textit{Deep Neural Network} (DNN). Despite this introduction being general, it is not intended to be complete: only the tools relevant for the subsequent chapters are here developed.\\
In the section \ref{section1.1} we explain foundations of neural networks, with the aim of showing how to build a simple DNN and how to train it on data.\\
In the section \ref{section1.2} we talk about some, in a certain sense more practical, aspects of constructing a DNN, which in turn really make the difference in making the algorithm effective. Indeed it turns out that in order to build up a DNN which actually performs well it is necessary to consider with care: the tuning of \textit{hyperparameters}, that are the parameters which are not being optimized by the neural network, the problems arising from over/under-fitting of the data and the techniques used to deal with them, which go under the name of \textit{regularization}, as well as different optimization algorithms, which can become very relevant in order to not being stuck in a local minima.\\

\section{Architecture and operation of a Deep Neural Network}\label{section1.1}
The very fundamental unit which compose every neural network is the \textit{node} or \textit{neuron}, that is a structure that takes some input features, performs a specific transformation on them, and gives the output:
\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/node}
\caption{}\label{fig1}
\end{figure}
\noindent To gain an intuition of what a node is really doing, it's useful to get a more precise idea of a possible problem we could want to face with our node and of an appropriate map $f:X\rightarrow Y$ we might want to use.
\begin{Example}\label{example1}
Consider a given dataset $\{(x_i,y_i)\}$ like the one in the next picture, and suppose to have to find an appropriate relation in the form $y=f(x)$ in order to predict the value of the variable $y$, even for unknown $x$ which we might encounter in the future.
\begin{figure}[H]
	\centering
	\includegraphics[width=7.5cm]{img/regression1}
	\caption{}
\end{figure}
\noindent As widely known, linear regression can be a first good answer to the problem, so, once performed the calculations, we are able to write:
\[
y=wx + b
\]
for some $w,b$; let's call this linear transformation $L\!: \, Lx=wx+b$. Now suppose that the output $y$ has a constrain, e.g. that $y\ge0$ must hold\footnote{The reason why non linear function like the one arising from this constrain are needed in deep learning will be seen in the end of this section.} $ \forall y $. Then it would be reasonable to update the $f$ function as in the figure below:
\begin{figure}[H]
	\centering
	\includegraphics[width=7.5cm]{img/regression2} 
	\caption{}
\end{figure}
\noindent Which mathematically means:
\[
y=max\{0,wx+b\}
\]
So we have that the response function $f$ is given by the composition of a linear function, $L$, and a non linear one, $A\,\,s.t.\,\, x\overset{A}{\longrightarrow}max\{0,x\}$, where of course every function is defined from $X$ to $Y$:
\[
f:X\rightarrow Y\quad s.t.\quad f=A\circ L
\]
In the deep learning literature the non linear function acting after the linear one is called \textit{Activation function}, while the specific one used in this example, namely $x\rightarrow max\{0,x\}$, is known as \textit{ReLU} function, which stands for \textit{Rectified Linear Unit}.$\qed$
\end{Example}
\noindent An approach like the one presented in example \ref{example1} unload all the complexity of a problem on the function $f$ that maps the given data to the predicted output; it thus become early less feasible as complexity grows. It's here that the neural network paradigm come in.\\
The main ideas behind it can be divided in two: a ``divide et impera''-like approach and a statistical-like one, based on random functions and optimization of appropriate functionals.\\
For what concerns the first, the idea is to stuck together different nodes in order to be able to reconstruct complex behaviors as the composition of simpler ones. This assembly of nodes is performed in two fashions: from one side we can imagine to feed in the data to different neurons, which will calculate their own output, and then to put together the outputs, building up what in the literature is called a \textit{layer} of nodes. From the other side we can put different layer in sequence, so that the first layer takes the input from the data, the second layer takes as input the output of the first one, and so on, until the last layer outputs the predicted y.\footnote{Technically it's more correct to say ``every node in the $n\!-\!th$ layer takes the input from every node in the $(n+1)\!-\!th$ layer'', but it's preferred to use the short sentence above when it's clear enough.}\\
Following the literature we call \textit{Hidden layer} every intermediate layer, as in the following picture.
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{img/neuralnet}
\caption{}\label{neuralnet}
\end{figure}
\noindent In deep learning the above mentioned process that brings the input to the output, passing by all the intermediate layers, is called \textit{forward propagation}, or, in short, forprop, and constitute the first part of the training algorithm of a neural network.\\
\newline \noindent One surprising aspect of neural networks is that they don't require their designer to decide which node of the first layer takes which part of the input, nor which nodes of the $n\!-\!th$ layer a generic node of the $(n+1)\!-\!th$ layer should consider, neither how much importance any node should give to each of its inputs. In fact, he gives all the inputs to every node, and let the neural network find it out by itself. In other words, given enough training example, i.e. enough $(x_i,y_i)$ known couples, neural networks are remarkably good at figuring out how to map unknown $x$ to the right $y$.\\
This capability is achieved by means of the second main idea of neural networks: what is called \textit{back propagation}, or backprop.\\
Let's consider again the single node as in figure \ref{fig1}, with $y=ReLU(wx+b)$. More precisely in this case we have:
\begin{equation}\label{linear}
L=\bm{w \cdot x} + b
\end{equation}
where $\bm x = (x_1,x_2,x_3)^T$, $\bm w$ is the vector of the \textit{weights}, since the dot product in \eqref{linear} can be seen as a weighted sum of the inputs, while \textit{b} is called \textit{bias}, since it affects this sum with a its contribution. \\
The basic idea is to perform a forward propagation with known data and randomly initialized parameters $\bm w,b$, then to calculate an appropriate distance between the predicted output, let's call it $\hat y$, and the known output\textit{y}.
This distance measure the \textit{cost C}, or the loss, of our computation.\\
Let's consider for simplicity:
\[
C=\frac{1}{2}(y-\hat y)^2
\]
Now we begin the backprop phase: we calculate the gradient of \textit{C} w.r.t. our parameters, and then we update them using an optimization algorithm aiming at minimizing the cost \textit{C}.\\
For example using gradient descent:
\begin{equation}\label{grad_desc}
\begin{cases}
\bm w \leftarrow \bm w - \alpha \frac{dC}{d\bm w} \\
b \,\,\leftarrow b\,\, - \,\,\alpha \frac{dC}{db}
\end{cases}
\end{equation}
where $\alpha$ is a, typically small, positive real number, which controls how fast we update our weights, and is thus called \textit{learning rate}.\\
The learning process is thus construct as a loop composed by forward propagation followed by backward propagation, with this sequence being repeated until the cost reaches a satisfying value.\\
Coming back to a full DNN, i.e. to a neural network with more than one hidden layer like the one in figure \ref{neuralnet}, it's easy to generalize the above algorithm.\\
Let's suppose we have a large enough dataset of $x_i$ with the corresponding $y_j$, and divided it in two disjoint subset, the \textit{training set} and the \textit{test set}.\\
We initialize all the weights randomly, and we start the loop, called \textit{training loop}, over the training set.\\
First we have forward propagation:
\begin{itemize}
\item every node of the input layer takes all the data as input, performs an affinity like the one in \eqref{linear} using it's own weights and bias, non-linearize the result through an activation function like the ReLU, and sends it's output to every node of the next layer;
\item every node of the hidden layers takes as input all the outputs coming from the previous layer, performs a weighted sum of them and adds a bias, using it's own parameters, and outputs the activated result to each node of the next layer;
\item the nodes in the last layer usually calculate only the linear transformation and outputs the predicted result.
\end{itemize}
At this point we calculate the value of the cost functional, and then we start backprop, in which every node, starting from the ones inside the last layer, calculate the gradients of the cost w.r.t. it's own parameters $w,b$ and updates them using an algorithm like gradient descent. Once we have updated all the parameters till the first layers we do another iteration of the algorithm.\\
When we are satisfied of the performance of our neural network, we do \textit{only one} forward propagation step, keeping the optimal parameters, but using the test set, and we evaluate the accuracy of the results.\\
\newline
\noindent Before going on with the discussion of deeper aspects of deep learning, we show why some of the choices made in the design of the neural network and of the algorithm are appropriate.\\
First of all it is important to notice that non linear activation function are in general necessary. In fact, consider a general DNN, identify with the subscript $l$ the quantities relative to the \textit{l}-th layer, call $n_d$ the number of the data in the set, $n_l$ the number of nodes in the layer l and suppose that $A$ is the identity for each node. We then have:
\[
A_l = L_l = W_l A_{l-1} + b_l
\]
where $A_{l}$ denotes the $n_l \times n_d$ matrix of the outputs of the \textit{l}-th layer, $W_l$ is a $n_{l-1} \times n_l $ matrix, and $b_l$ is a $n_l$ vector. Then we have:
\[
A_l=W_l(W_{l-1}A_{l-2} + b_{l-1})+ b_l = W_lW_{l-1}A_{l-2}+W_lb_{l-1}+b_l =: W'_l A_{l-2} +  b'_l
\]
By recursion is then easy to prove that in the end with a complete step of forward propagation we are computing only a linear combination of the initial data, as we would do in a much simpler way with just one node.\\
In this report we focus mainly on $tanh(\cdot)$ and ReLU activation functions, which are both very common choices, but others are being investigated as well.\\
For similar reasons it is very important to initialize the weights randomly: if we don't, and, for example, we initialize every parameter to a given value, then at the very first iteration all the nodes in the same layer are computing the very same output, and moreover are being updated at the same way during backward propagation. In the end this result in having a DNN that produces the same output of one which has just one node per layer.\\
Finally, it's worth noting that deep neural networks turn out to be more effective than bigger shallow neural networks in reconstructing and predicting complex behavior, especially when it is decomposable in a hierarchical grade of complexity, and that a minimum number of hidden layer is sometimes necessary. We don't prove it, but we will give a quantitative result of a comparison in section \ref{section2.2}, despite in a specific case.


\section{Further insights}\label{section1.2}
When designing a neural network, a great number of choices have to be taken: the number of layers, of nodes, the value of the learning rate, the kind of activation function... Moreover what can be a good setting in one field usually isn't that good in another, making DNN programming a very iterative process. \\
Before getting into it, it's useful to split the data in three sets: the training set, containing the most of the data, the development set and the test set. Then, the first thing to do is to evaluate the performance of the network on the training set: if the error on it is high it means that the network is underfitting the data\footnote{This situation is known in the literature as ``high bias".}, and increasing the size of the network or training it longer could be possible tries. Once the error on the training set is low enough we look at the performance on the dev set. If it is low we've done, otherwise it means that in the previous stage we have overfitted the data\footnote{Also described as a ``high variance" situation.}, that is we have tuned the DNN so much on the training data that it has difficulties to generalize to new ones. In this case we can try to get more data and/ore to use regularization techniques, which are the topic of the next paragraph. It's important to notice that once this modification are done, we've to start again from the beginning and check if the error on the training test is still low enough.\\
Once we have reached a reasonably good result on bot sets, we can finally verify the robustness of our network on the test set, which will give us an unbiased estimation, not having been used still.\\
This iterative process is resumed in the following flowchart:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{img/flowchart}
	\caption{}\label{flowchart}
\end{figure}
\noindent Another difficulty comes from the fact that in general the cost functional which has to be minimized is not convex, thus it may have local minima, and the space in which it lives, the one generated by the hyperparameters $W,b$ becomes early high dimensional, enhancing the number of plateaus, and a simple optimization algorithm might stuck in both of them (and it usually do so). Some work can be done on the learning rate and on choice of the optimization algorithm, as it is shown in paragraph \ref{par1.2.2}.\\

\subsection{Regularization}\label{par1.2.1}
There are many kind of regularization techniques, all aiming at reducing the overfitting on the training data. The most common is the $L_2$ \textit{regularization}, which consist in adding a term containing the norms of the parameters to the cost functional:
\[
C \longrightarrow C + \frac{\lambda}{2n_L}\left( \sum_{l=1}^{L}||W_l||_F + ||b_l||_2 \right)^2
\]
where $||\cdot||_F$ indicates the Frobenius norm, $L$ the number of layers and  $\lambda$ the regularization parameter. $L_2$ regularization has the effect of keeping the parameters relatively small, and thus from one side it is somehow as if we had a smaller neural network, and from the other side, being:
\[
A_l=W_lA_{l-1}+b_l
\]
helps keeping the layers more linear, since the activation functions usually have a \textit{tanh}-like form. In both cases $L_2$ regularization force the network to take simpler ``decisions", reducing so the overfitting.\\
Another famous technique is \textit{dropout}, which consist in selecting a different random fraction of the nodes at the beginning of every training iteration, and turning them off by removing the figure \ref{neuralnet} links. In this way if we look at a single iteration we are working exactly with a smaller network, and so we are reducing overfitting, but at the same time at every iteration we are using a different network, and so we are keeping the capability of the original bigger network to fit the complexity of the data.\\
Of course, while dropout may be useful to find weights able to generalize to data they've never seen, it is meaningful only during the training - development phase of the process, while it hasn't to be used on the test set.

\subsection{Optimization}\label{par1.2.2}
A large number of famous algorithms  in deep learning are based, among the others, on the generalization of the moving averages concept, that is here recalled.\\
Consider a set of data and, for simplicity, figure them as points in a 2D plane; suppose that their pattern follows somehow a certain curve, but in a noisy way. If we want to obtain that smooth curve we can't just take their average, otherwise we'll get a constant line, but we can imagine to cluster them and then to take averages. In order to recover a smooth function, consecutive clusters have to overlap. In practice we can proceed in this way\footnote{We use the notation that we will find when we'll introduce Adam as in \cite{Kingma}. Here $\theta$ is the data. }:
\begin{equation}\label{moving_averages}
\begin{cases}
m_0=0\\
m_t=\beta m_{t-1} + (1-\beta) \theta_t, \quad\quad 0\le\beta\le 1, \, t\geq 1
\end{cases}
\end{equation}
Let's suppose we choose $\beta=0.9$, then roughly speaking \eqref{moving_averages}  computes at every step the average of the last 10 values, producing a quite smooth curve that fits the pattern of the data.\\
\newline \noindent We now introduce four algorithms: gradient descent with momentum, RMS-prop, Adam, which is based on the previous two and that from its presentation at International Conference on Learning Representations in 2015 has became increasingly popular, and its infinity-norm variant AdaMax.\\
\newline \noindent Gradient descent with momentum is the direct application of \eqref{moving_averages} to standard gradient descent. For every iteration t:\footnote{For ease of notation, in the following the $t$ subscript is omitted and the gradient of the objective w.r.t. to a variable it's indicated as d(variable). }
\begin{itemize}\label{momentum}
	\item calculate dW, db
	\item update the momenta: 
	\begin{equation}
	\begin{cases}
	m_{dW}\longleftarrow\beta m_{dW} + (1-\beta) dW\\
	m_{db}\longleftarrow\beta m_{db} + (1-\beta) db
	\end{cases}
	\end{equation}
	\item update the parameters:
	\begin{equation}
	\begin{cases}
	W\longleftarrow W - \alpha m_{dW}\\
	b\longleftarrow b - \alpha m_{db}
	\end{cases}
	\end{equation}
\end{itemize}
The effect of the moving averages on gradient descent is a reduction of the oscillations in the path towards the optimum value of $W,b$, which in turn results in a faster convergence: as oscillations go to zero, the whole step is taken in the right direction. In other words, imagine a 2d plane with some parameters that we are optimizing on as orthogonal axis; then the situation before the addition of the momentum might be portrait as a triangle wave, instead performing averages the vertical oscillations cancel out themselves, while, if the algorithm converges to a point, the displacements to right sum up each other.\\
\newline \noindent RMSprop pursue the same aim, namely fastening convergence of gradient descent when there is an oscillating like behavior, but in a slightly different way:
\begin{itemize}\label{RMSprop}
	\item $\forall t$ calculate dW, db
	\item update the momenta: 
	\begin{equation}\label{rms_prop_s}
	\begin{cases}
	s_{dW}\longleftarrow\beta s_{dW} + (1-\beta) dW^2\\
	s_{db}\longleftarrow\beta s_{db} + (1-\beta) db^2
	\end{cases}
	\end{equation}
	\item update the parameters:
	\begin{equation}\label{rms_prop_update}
	\begin{cases}
	W\longleftarrow W - \alpha \frac{dW}{\sqrt{s_{dW}}}\\
	b\longleftarrow b - \alpha \frac{db}{\sqrt{s_{db}}}
	\end{cases}
	\end{equation}
\end{itemize}
where the superscript $2$ has to be intended element wise, and $s$ stands for square.\\
To visualize the idea behind RMSprop let's suppose we have $dW>>db$; then in every gradient descent iteration it is as if we are taking a great step in the \textit{W} direction, and a significantly smaller one in the $b$ direction. But with RMSprop, if \textit{dW} is big and \textit{db} is small, being $\beta$ usually small, we have in turn $s_{dW}$ big and $s_{db}$ small in \eqref{rms_prop_s}, so in \eqref{rms_prop_update} we take comparable steps in both directions.\\
As anticipated, Adam basically puts together this two algorithms, and performs a \textit{bias-correction} of the moving averages. The reference for this algorithm is the article by \cite{Kingma}, here we sketch it:
\begin{itemize}\label{Adam}
	\item gradient descent with momentum step:
	\begin{equation*}
	\begin{cases}
	m_{dW}\longleftarrow\beta_1 m_{dW} + (1-\beta_1) dW\\
	m_{db}\longleftarrow\beta_1 m_{db} + (1-\beta_1) db
	\end{cases}
	\end{equation*}
	\item RMSprop step:
	\begin{equation*}
	\begin{cases}
	v_{dW}\longleftarrow\beta_2 v_{dW} + (1-\beta_2) dW^2\\
	v_{db}\longleftarrow\beta_2 v_{db} + (1-\beta_2) db^2
	\end{cases}
	\end{equation*}
	\item momenta correction:
	\begin{equation*}
	\begin{cases}
	m_{dW}^{corr}=\frac{m_{dW}}{1-\beta_1^t},\,\, v_{dW}^{corr}=\frac{v_{dW}}{1-\beta_2^t}\\
	m_{db}^{corr}=\frac{m_{db}}{1-\beta_1^t},\,\, v_{db}^{corr}=\frac{v_{db}}{1-\beta_2^t}
	\end{cases}
	\end{equation*}
	\item parameters update:
	\begin{equation*}
	\begin{cases}
	W\longleftarrow W - \alpha \frac{m_{dW}}{\sqrt{v_{dW}}+\varepsilon}\\
	b\longleftarrow b - \alpha \frac{m_{db}}{\sqrt{v_{db}}+\varepsilon}
	\end{cases}
	\end{equation*}
\end{itemize}
where $\varepsilon$ is a small positive number, used to prevent division by zero.\\
\newline \noindent AdaMax is a variant of Adam, that can be obtained re-formulating the second step by means of the infinity norm. An interesting aspect of this algorithm is that it turns out that doing so we can avoid to correct for initialization bias, as needed in Adam\footnote{See \cite{Kingma}.}. The steps are:
\begin{itemize}\label{AdaMax}
	\item gradient descent with momentum step:
	\begin{equation*}
	\begin{cases}
	m_{dW}\longleftarrow\beta_1 m_{dW} + (1-\beta_1) dW\\
	m_{db}\longleftarrow\beta_1 m_{db} + (1-\beta_1) db
	\end{cases}
	\end{equation*}
	\item infinity norm update:
	\begin{equation*}
	\begin{cases}
	u_{dW}\longleftarrow max\{\beta_2 u_{dW}, |dW| \}\\
	u_{db}\longleftarrow max\{\beta_2 u_{db}, |db| \}
	\end{cases}
	\end{equation*}
	\item parameters update:
	\begin{equation*}
	\begin{cases}
	W\longleftarrow W - \frac{\alpha}{1-\beta_1^t}\cdot \frac{m_{dW}}{\sqrt{u_{dW}}+\varepsilon}\\
	b\longleftarrow b - \frac{\alpha}{1-\beta_1^t}\cdot \frac{m_{db}}{\sqrt{u_{db}}+\varepsilon}
	\end{cases}
	\end{equation*}
\end{itemize}
\noindent \\Before leaving the optimization topic, it's worth mentioning the \textit{learning rate decay}, a technique which involves the progressive reduction of $\alpha$ and that is often used along with the optimization algorithms.\\
Consider for simplicity standard gradient descent. What happens actually is that the steps are quite noisy in the taken direction, not exactly as the previous mentioned triangle wave. This doesn't give big problems at the beginning of the algorithms, but as we approach the minimum we might end up wandering for a technically not finite amount of time about it, but taking to big step to really reach it. Reducing in an appropriate way the learning rate as the iterations increase in principle doesn't eliminate this problem (unless we exactly hit the minimum), but by sure lead us to wander in a tighter region around the minimum, which usually is a good enough result. Typical choices are:
\begin{itemize}
	\item $\alpha = \frac{1}{1-d*t}\cdot\alpha_0$, \textit{d} being a decay rate,
	\item $\alpha \propto \frac{1}{\sqrt{t}}\cdot\alpha_0$
\end{itemize}
where in both cases \textit{t} stands for the iteration number and $\alpha_0$ is the initial learning rate, but also a simpler piecewise constant decay can make the difference.\\
\newline \noindent In this chapter a significant number of hyperparameters has been explicitly introduced, each of which has to be properly tuned; moreover the list is not exhaustive, for example how to split the data into the three sets has to be decided too\footnote{This depends strongly on the total amount of data, e.g. it could range from $60\%-20\%-20\%$ respectively for train-dev-test sets when we have about $10^3$ data, to $98\%-1\%-1\%$ when we have millions of data, basically because the dev and the test set has to be just big enough to evaluate the performance of the network, while the training set is the one on which the network really \textit{learn} the optimal parameters. Moreover, when having such a big amount of data, it's better considering techniques based on mini-batches.}. Fortunately not all the parameters plays the same role: a good idea might be to start from hyperparameters like the learning rate, the number of hidden units and of nodes per layer, while other parameters like for example the default Adam ones ($\beta_1=0.9,\,\beta_2=0.999,\,\varepsilon=1e-8$) are already good for most applications. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Application to PDEs}\label{chapter2}
\setcounter{figure}{5}
The idea of applying machine learning to numerical analysis can be traced back up to \cite{Lagaris}, who tried to solve ODEs and PDEs by means of neural networks. The authors have shown some of the potential of DNNs, such as their being excellent interpolators and the ease of parallelization of the deep learning algorithm; moreover they proposed a first comparison with finite element method and faced the problem one encounters when dealing with BVPs in a deep learning setting, which we will expose too. However they still use a mesh, which is a not necessary burden as we'll show, and they used a shallow neural network, with only one hidden layer, that, albeit theoretically able to recover any continuous function, is quite far from being optimal.\\
More recently the explosion of the Big Data field has given rise to stronger interests in deep learning, leading to the development of new tools which are being applied in the study of PDEs too with promising results; for example \cite{Sirignano} face the problem of dimensionality, solving PDEs in space up to 200 dimension using a mesh-free algorithm.\\
Anyway research in this field is still very young, most of the papers having 1-2 years old, and coherently there isn't a solid general theoretical framework as the one of FEM; nonetheless there already are some interesting results, like the one of \cite{Jinchao}.\\
The aim of the present chapter is to outline possible ways of proceeding for problems that one most often encounters in engineering and applied sciences, taking as example the classical Poisson-Dirichlet problem, highlighting pros and cons of the method too (section \ref{section2.1}), and then to provide some useful theoretical result (section \ref{section2.2}), which compare DNNs accuracy w.r.t. FEMs.

\section{The Poisson-Dirichlet problem}\label{section2.1}
There are different ways to employ deep learning to solve the Poisson-Dirichlet problem:
\begin{equation}\label{PoissonDirichlet}
\begin{cases}
-\Delta u = f \quad\quad in\, \Omega\\
u_{|_{\partial\Omega}}=g\qquad on\, \partial\Omega 
\end{cases}
\end{equation}
Recently \cite{Kailai} have obtained good results in few dimension with well-behaved manufactured solutions; their strategy is based on \cite{Lagaris} for the treatment of the boundary condition and on \cite{Sirignano} for the mesh; this method follows.\\
Having in mind what has been said in chapter \ref{chapter1}, the first thing to do is to generate the datasets. Concerning this, there are at least two important things to notice: 
\begin{itemize}
	\item the point can be sampled randomly in $\overline{\Omega}$, so to wipe out the need of a mesh, guaranteeing in this way large computational savings, especially in high dimension;
	\item the probability that points given by a random generator lie on $\partial \Omega$ is technically zero, since $|\partial\Omega|/|\Omega|=0$, so it has to be ensured that the network is trained on the boundary too.
\end{itemize}
\noindent The authors splits the problem in two similar one, the first having a training set made only by boundary points, the second only by interior points; \footnote{The fundamental aspect is to have ``enough" boundary point. A conceptually simpler solution may be to force the random generator to output an appropriate fraction of boundary points over the total generated, and then to use only one network training on the whole set.}in both cases they use equation \eqref{PoissonDirichlet} RHS to calculate $f,g$ at the random generated points. Then they build up two neural networks, specifically with $3$ layers and $64\times 3$ nodes, $tanh(\cdot)$ activation function and Adam optimizer, to reconstruct the solution $u$ as:
\begin{equation}\label{sol_kailai}
u(x,y;w_1,w_2)=A(x,y;w_1)+B(x,y)\cdot N(x,y;w_2)
\end{equation}
where \textit{A} is the output of the neural network approximating the boundary condition, \textit{N} the one approximating the solution in the domain, and \textit{B} a function that goes to zero on the boundary; consequently their cost functional has the form:
\begin{equation}\label{cost_kailai}
\sum_{i=1}^m\left((g_D)_i - u(x_i,y_i)\right)^2 + \sum_{i=1}^n\left(f(x_i,y_i) - Lu(x_i,y_i)\right)^2
\end{equation}
$L$ being the differential operator in strong form.\\
More precisely their algorithm has two nested loops: for every iteration of the interior network, they perform a full training of the boundary network. Using a $sin(\cdot)sin(\cdot)$ manufactured solution on the unit square with their code, freely available on Github, is possible to recover a very good solution in few hundreds of iterations\footnote{Code and report are here: \cite{Kailai}; notice that it requires Python2 and TensorFlow. Figure \ref{pd1} can be found at the same site.}:
\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{img/pd1.png}
	\caption{}\label{pd1}
\end{figure}
\noindent However their algorithm does not perform equally good nor in higher dimension neither with ill-behaved solutions, and further developments are needed.\footnote{Further insights and possible reasons are proposed in the next section.}\\
\newline \noindent A very different approach, but again based on deep learning, has been proposed by \cite{Weinan}. Here it is reported in the case regarding the problem under analysis, in order to show both the largeness of the range of possible DNN based methods for solving PDEs and their common traits, and to point out a very interesting link between this approach, weak formulations and optimal control theory.\\
In their article the homogeneous Poisson-Dirichlet problem is re-casted analytically in the corresponding optimal control problem, leading to a cost functional of the form:
\begin{equation}\label{cost_weinan}
\int_{\Omega}\left(\frac{1}{2} |\nabla u|^2 - uf \right)dx + \beta \int_{\partial\Omega}u^2 d\sigma
\end{equation}
which, besides the positive constant $\beta$ and considering homogeneous Dirichlet conditions, turns out to be the exactly the weak form of the \cite{Kailai} cost functional.\\
The method then proceeds minimizing the cost functional \eqref{cost_weinan} over the set of admissible functions $U_{ad}$, and is here that deep learning comes in. \\
In fact functions in $U_{ad}$ are re-constructed through neural networks, so moving from an additive-like construction proper of standard approximation theory to a compositional-based one.\\
At this point the authors apply a quadrature formula to the integral in \eqref{cost_weinan} and then solve the final optimization problem with a proper optimization algorithm.\\\\
\noindent The algorithm produces good results but, as other approaches based on DNN, has a drawback side: even when the initial problem is convex, the variational problem which is then obtained isn't so, inheriting thus the problem of local minima and saddle points proper of deep learning.

\section{Some more in-depth theoretical results}\label{section2.2}
In this paragraph some results, obtained mainly by \cite{Jinchao}, are presented, with the aim of giving estimates of the optimal size for a DNN employed to solve PDEs; in particular the results found are useful to ensure that the DNN recovers the same accuracy of the finite element method based on linear base functions.\\
We recall some FEM notation we use in the following: suppose to have a bounded domain\footnote{In the sense of open and connected.} $\Omega\in\mathbb{R}^d$ ad a conforming mesh or grid $\mathcal{T}_h\subset\Omega$, define moreover $k_h$ as the maximum number of mesh elements neighboring a grid point; then the FE space of grade one is:
\[ V_h=\{ v \in C_\Omega \text{ s.t. $v$ is linear on every } \tau_k \in \mathcal T_h \} \]
In other words, the base functions of the grade one FEM are continuous piecewise linear functions, in short CPWL; it is thus natural to proceed into the study using ReLU-DNNs, since the ReLU activation function is a CPWL function and DNNs construct their output as a composition of linear functions and activation functions, giving so a CPWL function as output.\\
\newline \noindent The two main results are\footnote{These are taken respectively from Theorem 3.1 and Corollary 5.1 of \cite{Jinchao}.}:
\begin{Theorem}\label{thm3.1}
	\textit{Given a locally convex finite element grid $\mathcal T_h$ , any linear finite element function in $\mathbb R^d$ with N degrees of freedom can be written as a ReLU-DNN with at most $\lceil log_2 (k_h)\rceil +1$ hidden layers and at most $\mathcal O(k_hN)$ neurons.}
\end{Theorem}
\begin{Theorem}\label{corol5.1}
	\textit{Given a locally convex finite element grid $\mathcal T_h$ , any linear finite element function in $\mathbb R^d$ with N degrees of freedom can be written as a ReLU-DNN with at most $\lceil log_2 (d+1)\rceil$ hidden layers and at most $\mathcal O(d2^{(d+1)k_h}N)$ neurons.}	
\end{Theorem}
\noindent A direct comparison of the two theorems shows that although it is possible to achieve linear FEM like accuracy with a relatively shallow network, the deeper one has a considerably smaller size.\\
To fix the ideas it has been considered $d=2$ and $\Omega$ being the unit circle. Then with a standard triangulation of the domain is:
\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{img/triangulation.png}
	\caption{}\label{k_h}
\end{figure}
\noindent To do an estimate it's thus possible to assume $k_h=6$.\\
To find a proper value for N we consider the very well-behaved solution:
\[u_{exact}=e^{-(x^2+y^2)}(x^2+y^2-1)\]
of the problem \eqref{PoissonDirichlet} obtained computing $f$ as $\Delta u_{exact}$; we then solve the problem different times using the FOSS software \cite{freefem++} refining the mesh until a good result is reached.\footnote{Code in \ref{appendix}}. \\
We obtain a relative $L_2$ error, defined as $err_{L_2} = \frac{||u_{exact}-u_h||_2}{ \frac{1}{2} (||u_{exact}||_2+||u_h||_2) }  $, of $1.65\%$ when using 36 triangles on the border of the domain, as in figure \ref{k_h}, which lead to a $V_h$ space with 137 degrees of freedom:
\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{img/u2.png}
	\caption{}\label{u2}
\end{figure}
\noindent In the end is possible to conclude that to recover the same result using a ReLU-DNN \textit{at most}:
\begin{itemize}
	\item $\lceil log_2 (k_h)\rceil +1=4$ layers and $\mathcal O(k_hN)\simeq 10^3$ total nodes, or
	\item $\lceil log_2(d+1) \rceil = 2$ layers and $\mathcal O(d2^{(d+1)k_h}N)\simeq 10^8$ total nodes
\end{itemize}
are needed, confirming that a deep neural network is computational more convenient that a shallow one\footnote{This is coherent with the fact that the power of the neural networks approach stands in their remarkable capability of approximate non linear functions, which is achieved by means of composition between affinities and non linear activation functions, and the step of activation occurs in the forward propagation as many times as the number of layers.} and, perhaps more importantly, giving a quantitative estimate of this convenience.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Implementation of neural-net}\label{chapter3}
\setcounter{figure}{8}
In this chapter an in-depth view of a Deep Learning C++ program, called \textit{neural-net}, is given. As just shown in chapter \ref{chapter2}, what different DNN based PDE solvers have in common is their relying on whats turn out to be DNN's numerical analysis strongest point: their remarkably capability of reconstructing function, which is used to construct the basis of the approximation spaces. For this reason it has been decided to start writing an interpolator based on DNNs; different possible expansions are being investigated as well.\\
The program has been built using the \cite{git} distributed version control system, with GitHub as hosting service, and can be freely cloned or downloaded from the web page: \href{https://github.com/pjbaioni/neural-net}{\emph{https://github.com/pjbaioni/neural-net}}.\\
In order to build and run the program, some not included dependencies are needed, in particular a C++ compiler, \cite{make}, \cite{gnuplot}, \cite{eigen} and \cite{boost} libraries, while to build the documentation a TeX compiler has to be used; more details are given in the README.md file and in chapter \ref{chapter4}, where it is shown how to build the program and run an example. 

\section{Directory tree}
Downloading the repository ``neural-net'' one finds:
\begin{itemize}
	\item the \textit{data} folder, which contains the input and output data in .dat and .pot (for \cite{getpot}) format, \cite{gnuplot} scripts in .gnu format, and their graphical output in .png format;
	\item the \textit{doc} folder, containing this documentation in .tex and .pdf format, plus the \textit{img} sub-folder where the images included by the TeX file are stored;
	\item the \textit{include} folder, containing the headers \textit{GetPot.hpp}, an utility used for parsing parameters file and command line options, \textit{gnuplot-iostream.hpp}, an utility used to output a graphical representation of the results in an interactive way, \textit{NeuralNetwork.hpp}, which holds the NeuralNetwork \textbf{class}, and \textit{Optimizers.hpp}, where all the optimizers employable from NeuralNetwork are defined as \textbf{class templates};
	\item the \textit{src} folder, which contain the source files main.cpp, NeuralNetwork.cpp, where the NeuralNetwork class member functions are defined, the Makefile which can be used to automatic build the main program, and the \textit{write\_set} sub-folder, where the \textit{write\_set.cpp} file used to generate datasets is placed;
	\item the COPYING file, a plain text file containing copying informations and licenses;
	\item the README.md file, a markdown file containing basic informations;
	\item the (hidden) \textit{.gitignore} file, that traces the file extensions which are not being pushed to the remote, mainly compilation files, executables and comments file.
\end{itemize}
\noindent Despite their different extensions, every non-png nor non-pdf file can be opened by any text editor.
\section{The Neural Network class}
The class NeuralNetwork implements a computational efficient design of a deep neural network (DNN).\\
As seen in chapter \ref{chapter1}, to which we refer for the names of the principal quantities as for the justification of the main algorithms, a DNN is composed by different layers, each of which is composed by a (possibly) different number of nodes. At the beginning of the present project, this hierarchical structure naturally inherent to DNNs led to the idea of developing three classes, NeuralNetwork, Layer and Node. For reason that will appear clearer later on\footnote{Basically, because first, internal and last layer performs slightly different works.}, Layer would have been an abstract class, from witch the child classes InputLayer, HiddenLayer and OutputLayer would have inherited. That approach doubtless had the pro of adhering to the abstract model of a DNN, from the more general structure to the really low-level computations, but initial tests have shown that it wasn't so competitive from a computational efficiency point of view, besides being more complex than strictly necessary.\\
The final choice has been to ``vectorize'' the computations as much as possible, grouping together all the operations performed by nodes in each layer by means of matrices.
Of course this passage, which can be considered computationally mandatory in machine learning typical languages such as Python, hasn't brought efficiency by itself, being C++ a compiled language, but has allowed to rely on strongly optimized well known linear algebra libraries, which have made the difference. In particular the choice is fallen on the \cite{eigen} template library, for its immediate compatibility with C++ and its ease of use and linking with respect to other classical libraries such as Lapack, while maintaining a very good performance.
Having reached this point, and having loose somehow an eye-apparent evident sight on each node operations, keeping the original design of a neural network class composed by three different specifications of the layer class has appeared senseless, and thus it has been decided to simplify the code making the neural network class hold just vectors of the quantities that characterized every layer. \\
The choices made have thus led to a structure as the following:
%basicstyle=\fontsize{9}{11}\ttfamily
\begin{lstlisting}[frame=single, name=neuralnet, showstringspaces=false]
class NeuralNetwork{

private:

	//Architecture hyperparameters:
	size_t nlayers;
	VectorXs nnodes;		

	//Parameters to be optimized:
	vector<MatrixXd> W;	
	vector<VectorXd> b;		

	//Forward propagation outputs:
	vector<MatrixXd> L; 	
	vector<MatrixXd> A;		

	//Back propagation gradients:
	vector<MatrixXd> B;		
	vector<MatrixXd> dW;
	vector<VectorXd> db;

	//Optimizers:
	vector<shared_ptr<GradientDescent<MatrixXd>>> W_optimizer;
	vector<shared_ptr<GradientDescent<VectorXd>>> b_optimizer;

public:

	//Constructor:
	NeuralNetwork(const VectorXs &);
	
	//Training function:
	void train(const MatrixXd & Data, double alpha, size_t niter, double tolerance, const size_t W_opt, const size_t b_opt, const size_t nrefinements=1, const bool verbose=false);
	
	//Test function:
	pair<VectorXd,double> test(const MatrixXd & Data);
	
};

\end{lstlisting}
\noindent Note that here and in most of the following codes all preprocessor's directives as includes and header guards, as well as scope operators, typedefs and comments have been omitted for brevity.\footnote{See NeuralNetwork.hpp for the complete version.}\\
So the class NeuralNetwork hold basically four kind of data members: 
\begin{enumerate}[(i)]
	\item\label{item:i} architecture (hyper)parameters, like {\ttfamily nlayers} and {\ttfamily nnodes}, which specify respectively the total number of layers and the number of nodes for each layer, {\ttfamily VectorXs} being a {\ttfamily typedef} for an Eigen dynamic vector holding {\ttfamily std::size\_t}s, miming the one of {\ttfamily Eigen::VectorXd};
	\item\label{item:ii} the parameters to be optimized, namely {\ttfamily W} and {\ttfamily b}, that has to be somehow a vector of vector, i.e. not a matrix, since the number of nodes varies for each layer;
	\item\label{item:iii} the \textit{propagation members} {\ttfamily L}, {\ttfamily A}, {\ttfamily B}, {\ttfamily dW} and {\ttfamily db}, that holds the data computed at every cycle of forward and backward propagation;
	\item \label{item:iv} the optimizers for {\ttfamily W} and {\ttfamily b}, which hold a (smart) pointer to the base class of the Optimizers class hierarchy.\footnote{Optimizers.hpp is illustrated in section \ref{section3.3}}
\end{enumerate}
For what concerns the (\ref{item:i}) group, {\ttfamily nnodes} is the \textit{true} parameter, indeed it is the only argument that the constructor takes, while {\ttfamily nlayers} is stored for convenience, since it is used very often in for loops in the member functions, but could be removed. In general, it has been chosen to use {\ttfamily std::size\_t} every time an unsigned was needed, to adopt a standard that would fit every necessity.\\
As explained above, {\ttfamily W} and {\ttfamily b} are the vectorized version of \ref{chapter1} parameters: if we call {\ttfamily l} the generic layer, then {\ttfamily W[l]} is an {\ttfamily nnodes(l)}$\times${\ttfamily nnodes(l+1)} matrix, while {\ttfamily b[l]} is a {\ttfamily nnodes(l+1)} column vector, as in Figure \ref{neuralnet}.\\
A similar comment holds for the propagation members (\ref{item:iii}), with two observations:
\begin{itemize}
	\item forward members, {\ttfamily L} and {\ttfamily A}, have been chosen to be the most general possible, even if for the problems for which the program has been designed the first one and the last one will be always column vectors, and not true matrices;
	\item backward members, {\ttfamily B} (which stands for ``backward''), {\ttfamily dW} and {\ttfamily db}, aren't strictly needed, like the previous {\ttfamily nlayers}, but simplify significantly the subsequent code.
\end{itemize}
The (\ref{item:iv}) group has been introduced later, indeed simple algorithms like Gradient Descent can be applied directly during the training phase, i.e. can be written directly by hand in a {\ttfamily training()} function. However it turns out that most finer optimizers need to store estimators of the gradients, and this estimators have to be updated at every training iteration, thus the need for storing them in the neural net.\\
Since, as it will be seen in the following section, optimizers have been implemented applying inheritance, here we store smart pointers to the base class, instead of the proper optimizer itself, in order to be able to use polymorphism later and to let the user choose the desired optimizer runtime, when calling the training function.\\
\newline \noindent
Having introduced the data members, consider the member functions:
\begin{lstlisting}[frame=single, showstringspaces=false]
NeuralNetwork(const VectorXs & nn): nlayers(nn.rows()),nnodes(nn){

	W.reserve(nlayers-1);
	b.reserve(nlayers-1);
	for(size_t l=0; l<nlayers-1; ++l){
		W.emplace_back(MatrixXd::Random(nnodes(l),nnodes(l+1)));	
		b.emplace_back(VectorXd(nnodes(l+1)));
	}

	L.reserve(nlayers);	
	A.reserve(nlayers-1);

	B.reserve(nlayers-1);	
	dW=W; 
	db=b; 

	W_optimizer.reserve(nlayers-1);
	b_optimizer.reserve(nlayers-1);
}
\end{lstlisting}
The constructor initialize every member, in order of declaration. In all methods whenever possible efficiency has been considered: for example here {\ttfamily nlayers} and {\ttfamily nnodes} are initialized in the member initializer list, and {\ttfamily std:: vector<T>::reserve()} followed by {\ttfamily std::vector<T>::emplace\_back()} is preferred over {\ttfamily std::vector<T>::push\_back()}.\\
As already explained {\ttfamily W} has to be initialized randomly; for this purpose it has been chosen to use the Eigen {\ttfamily Random()} function, which returns a double uniformly distributed between zero and one. Of course different choices are possible, for example the ones based on the {\ttfamily <random>} header of the standard library. In particular, to be strictly random, as requested by the chapter \ref{chapter1} argumentation, we should use them and guarantee effective randomness including ``real entropy'' in the seed, for example in this way:
\begin{lstlisting}[frame=single, showstringspaces=false]
size_t sd = chrono::system_clock::now().time_since_epoch().count();
default_random_engine gen(sd);
uniform_distribution<double> random_value(0,1);
for(size_t i=0; i<W.rows(); ++i)
	for(size_t j=0; j<W.cols(); ++j)
		W(i,j) = random_value(gen);
\end{lstlisting}
However, the Eigen default random function has shown very good performances, so it has been preferred due to its simplicity.\\
It's worth noting that only {\ttfamily W}, {\ttfamily b} and their gradients can be fully initialized, since it has been chosen to take the input data, whatever it is, training or test data, chosen optimizer..., only when it is really used. Consequently variables like the number of data, which is one of the dimension of the {\ttfamily A[l], L[l]} matrices, is not known at this moment.\\
In every case the needed space in the vectors is known from {\ttfamily nlayers}, and thus is pre-allocated; only the strictly needed space is allocated, often only {\ttfamily nlayers-1}, an action that is possible due to the choices made in the following {\ttfamily train()} member function (scomposed in different frames for clarity):
\begin{lstlisting}[frame=single, showstringspaces=false]
void NeuralNetwork::train(const MatrixXd & Data, double alpha, size_t niter, double tolerance, const size_t W_opt, const size_t b_opt, const size_t nrefinements, const bool verbose){                     
	///////////////////
	//      Init     //
	///////////////////

	size_t ndata=Data.rows();

	//L has nlayers components:
	for(size_t l=0; l<nlayers;++l)
		L.emplace_back(ndata,nnodes(l));
	//A hasn't the last one:
	for(size_t l=0; l<nlayers-1;++l)
		A.emplace_back(ndata,nnodes(l));
	//B hasn't the first one:
	for(size_t l=1; l<nlayers;++l)
		B.emplace_back(ndata,nnodes(l));

	//First layer only reads the input:
	L[0]=Data.col(0); 
	A[0]=L[0];	
\end{lstlisting}
Since the input datasets have to be read somehow, and since the ``layer structure'' was already ready, the first layer has been reserved for reading only. This is of course a personal choice, motivated basically from the fact that it's easy and works good. A more elaborate one could have been to let the first layer weight the $(x,y)$ couples received during training from the dataset, and thus having e.g. {\ttfamily W[0]} an {\ttfamily ndata x nnodes[0]} matrix, but, being {\ttfamily ndata} quite large, this introduces by sure some overhead.\\
Since in the present version the first layer only reads the input, it don't need weights, biases and gradients at all, and for this reason their vectors have one component less.\\
A different discussion must be made with regards to the activated outputs variable {\ttfamily A}, which again has just {\ttfamily nlayers-1} elements, but for another reason, less subjective: since we want our output to be a function, in the sense of mathematical analysis, with no particular requirements, we cannot in general activate the last output, because if we would do so for example with a $tanh(\cdot)$, we would be able to reconstruct only $tanh(\cdot)$s.\\
The ``Init'' phase of the {\ttfamily train()} function finishes with the initialization of the optimizers, which is made through a switch-case construct:
\begin{lstlisting}[frame=single]
//Optimizers:
string W_opt_name, b_opt_name;
switch(W_opt){
	case 0:
		for(size_t l=0; l<nlayers-1; ++l)
			W_optimizer.emplace_back(make_shared<GradientDescent<MatrixXd>>(dW[l].rows(),dW[l].cols()));
		W_opt_name = "GradientDescent";
		break;
	case 1:
		for(size_t l=0; l<nlayers-1; ++l)
			W_optimizer.emplace_back(make_shared<GDwithMomentum<MatrixXd>>(dW[l].rows(),dW[l].cols()));
		W_opt_name = "GDwithMomentum";
		break;
	case 2:
		for(size_t l=0; l<nlayers-1; ++l)
			W_optimizer.emplace_back(make_shared<RMSprop<MatrixXd>>(dW[l].rows(),dW[l].cols()));
		W_opt_name = "RMSprop";
		break;
	case 3:
		for(size_t l=0; l<nlayers-1; ++l)
			W_optimizer.emplace_back(make_shared<Adam<MatrixXd>>(dW[l].rows(),dW[l].cols()));
		W_opt_name = "Adam";
		break;
	default:
		for(size_t l=0; l<nlayers-1; ++l)
			W_optimizer.emplace_back(make_shared<AdaMax<MatrixXd>>(dW[l].rows(),dW[l].cols()));
		W_opt_name = "AdaMax";
		break;
}		
	
switch(b_opt){
	case 0:
		for(size_t l=0; l<nlayers-1; ++l)
			b_optimizer.emplace_back(make_shared<GradientDescent<VectorXd>>(db[l].rows(),db[l].cols()));
		b_opt_name = "GradientDescent";
		break;
	case 1:
		for(size_t l=0; l<nlayers-1; ++l)
			b_optimizer.emplace_back(make_shared<GDwithMomentum<VectorXd>>(db[l].rows(),db[l].cols()));
		b_opt_name = "GDwithMomentum";
		break;
	case 2:
		for(size_t l=0; l<nlayers-1; ++l)
			b_optimizer.emplace_back(make_shared<RMSprop<VectorXd>>(db[l].rows(),db[l].cols()));
		b_opt_name = "RMSprop";
		break;
	default:
		for(size_t l=0; l<nlayers-1; ++l)
			b_optimizer.emplace_back(make_shared<Adam<VectorXd>>(db[l].rows(),db[l].cols()));
		b_opt_name = "Adam";
	break;
	case 4:
		for(size_t l=0; l<nlayers-1; ++l)
			b_optimizer.emplace_back(make_shared<AdaMax<VectorXd>>(db[l].rows(),db[l].cols()));
		b_opt_name = "AdaMax";
		break;
}	
\end{lstlisting}
Notwithstanding the possibility to choose different optimizer for {\ttfamily W} and {\ttfamily b} was foreseen only to investigate the relative importance of the two parameters, preliminary tests done trying to predict a wavepacket-like function has shown that the best compromise between accuracy and number of iteration is reached when using AdaMax for {\ttfamily W} and Adam for {\ttfamily b}, so these have become the default.\\
The {\ttfamily train()} function implements a piecewise constant learning rate decay, obtained in this way:
\begin{lstlisting}[frame=single]
///////////////////
//Training loops //
///////////////////
double old_cost{numeric_limits<double>::infinity()};
double cost{-1.}, err{old_cost};
niter=niter/nrefinements;
vector<size_t> backup_t(nrefinements);
	
for(size_t ref=1; ref<=nrefinements; ++ref){
	for(size_t t=1; t<=niter; ++t){
		//do one forward propagation
		//eventually output the cost
		//do one backprop
	}
//update refinement parameters for the next training loop:
alpha=alpha/10;
tolerance=tolerance/((10-2*ref)*10);
} 
\end{lstlisting} 
Learning rate decay has been implemented because it turned out to be very important to obtain acceptable accuracy. This implementation has the pros of being automatic, very simple and effective enough, but, since everything in DNN programming is very problem dependent and has to be tuned properly case by case, better results can be obtained miming this procedure directly in the main, when training the net. Indeed is enough to don't pass the number of refinements as argument to turn off the automatic learning rate decay, and then implement it manually with subsequent calls to the training function. Guidelines to do it are presented in chapter \ref{chapter4}.\\
The forward propagation step that take place in the above inner loop follows the general theory:
\begin{lstlisting}[frame=single]
/////////////////////////
// Forward propagation //
/////////////////////////
for(size_t l=1; l<nlayers-1; ++l){
	//Summing the column vector b (nnodes(l+1)x1) to each 
	//column of A*W (ndataxnnodes(l)*nnodes(l)xnnodes(l+1)) :
	L[l] = ( A[l-1]*W[l-1] ).rowwise() + b[l-1].transpose();
	A[l] = tanh( L[l].array() );
}	
//The final output shouldn't be activated, otherwise it will be necessarly a tanh:
L[nlayers-1] = ( A[nlayers-2]*W[nlayers-2] ).rowwise() + b[nlayers-2].transpose();
	
//Computing cost as the L2 distance: (divided by 2, for ease in later differentiation)
cost = .5 * (L[nlayers-1] - Data.col(1)).array().square().matrix().sum();
\end{lstlisting}
As evident, the code strongly relies on the Eigen built-in operations and function; in particular at line 7 and 11 of the above code a Python-like \textit{broadcasting} is implemented. An advantage of using Eigen is the ease of switching between matrix like operations, like the products in the same lines of code, to element wise operations, invoked with a {\ttfamily .array()}; moreover, when optimization is activated, such conversions, as well as transpositions and so on, are implemented so that they are costless from a computational point of view.\\
Then a convergence check and, eventually, an output to std::output is performed:
\begin{lstlisting}[frame=single, showstringspaces=false]
/////////////////////////
//  Convergence check  //
/////////////////////////
//Output the current cost
//and check if convergence is reached:
if( verbose && (t%25==0) ){
	cout<<"t="<<t<<" cost="<<cost<<" W_opt="<<W_opt_name<<" b_opt="<<b_opt_name<<" alpha="<<alpha<<"\n";
	err = abs(old_cost-cost) / ( (cost+old_cost)/2 );
	if(err<tolerance){
		backup_t.push_back(t);
		break;
	}
	else
		old_cost = cost;
}
\end{lstlisting}
Draft:\\
Backprop: at the beginning was from right to left to follow theory, but then i've turned it from left to right and just reverted the for condition, because it is very more convenient, see commit for details; moreover overloaded call operator was chosen before polymorphism because it could be used in a less ugly way than now, with no parentheses and dereference operator. Very few words on chain rule too. Gradient descent, which was used at the beginning, survive here as a comment for documentation purposes: it shows what whatever optimizer morally do.\\
Final output: few word on accumulate, i.e. it is used to print the total number of iterations, since it is reset every time a refinement is done.
I could have written something like: if(nrefinements==1) do certain things, else do others more, e.g. this step or having backup\_t.
However i've preferred to avoid this, since every if introduce a decision tree in the code, and thus some overhead, and i've written the code in such a way that it just works even with nrefinements==1. Of course in this case backup\_t is created for no reason, but i've preferred code brevity and execution time over memory saving.\\
Test: the test function performs just one forward propagation, with two big differences wrt train:\\
 - it doesn't store data(i.e. it uses only 2 matrices and it overwrites them, not 2 vectors of matrices), since it wouldn't use it anyway (when is for free, i save memory), \\
 - and thus it returns the result in the end\\
One could ask why don't use the already stored A,L anyway, so to don't even allocate the 2 matrices. In principle it could be done actually, but it would require more coding: A[l],L[l] have to be resized for every l, since in general ntraininingdata!=ntestdata, while i've always tryed to keep the code as simple as possible; by the way, that approach would introduce a little overhead too.
\newline\noindent Final remarks: as can be seen, the code, despite being 1d, can be extended to a n-d interpolator conceptually with no effort (of course, this potentially introduces a lot of choices, and so of time consuming new parameters tuning). The very simplest way it would be to make the first layer have a W (and a b, and so even backprop variables), basically to do a weighted average of x1,x2,x3...xn, in such a way that the second layer takes just an 1d x as usual (x=w1x1+w2x2...). In this way we are giving different (or equal, if we prefer), importance to every dimension. This method includes the already implemented one as a special case with n=1 and W=1.
Of course even in this case it would be possible to give different importance even to the single tuples of coordinates, as above we've said we don't do since results are already good, in 1d, as they are.
Moreover that ``squeezing'' from nd to 1d can be performed later on, in another layer. This choice by sure would introduce more overhead, but may result in less training iterations, since in this way we keep the dimensionality alive for more time.BUT  the more one enrich the code, and exponentially more tuning is needed, because not only this new choices have to be tested, but even their combination with the already existing hyper-parameters. It's thus clear that that would go over the aims of the present project.


\section{The Optimizers class templates}\label{section3.3}
\section{Write\_set and data}
\section{The main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Examples}\label{chapter4}

\section{Building instructions}
\section{Reconstruction of a wave packet}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Conclusions and further developments}
\addcontentsline{toc}{chapter}{Conclusions and further developments}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
\chapter*{Appendix}\label{appendix}
\addcontentsline{toc}{chapter}{Appendix}
The code used in section \ref{section2.2} follows:
\begin{lstlisting}[language=FreeFem++, frame=single, name=extimate_dof, showstringspaces=false]
// Code written to extimate the number of degrees 
// of freedom (dof) needed to reach a good extimate
// in section 2.2 of the report.

///////////////////////////////////
////////////Instructions///////////
///////////////////////////////////

/*	
To run the code, save it as and edp file: <filename>.edp
(Note that edp files can be opened by any text editor, saving
as .txt works as well)
Then in a terminal emulator do:
$ FreeFem++ filename.edp

The code has been tested on Debian 9.9,
equipped with FreeFem++ version  3.47 

pjbaioni 2020
*/

///////////////////////////////////
////////////////Code///////////////
///////////////////////////////////

// Mesh
int n=36; //n -> dofs: 9 -> 13, 12 -> 20, 16 -> 30 , 18 -> 41, 20 -> 45 ...
border c(t=0, 2*pi){x=cos(t); y=sin(t); label=1;}
mesh Th=buildmesh(c(n));
plot(Th,cmm="Mesh",wait=1);

// Space
fespace Vh(Th,P1);
cout<<"Degrees of freedom N = "<<Vh.ndof<<endl;

// Manufactured solution:
real a=1.;
mesh T=buildmesh(c(5*n));
fespace V(T,P2);
V uexact = exp(-a*(x^2+y^2))*(x^2+y^2-1);
plot(uexact,fill=1,value=1,cmm="u_exact_h",wait=1);

// Data
func f = -4*exp(-a*(x^2 + y^2))*(a^2*x^4 + 2*a^2*x^2*y^2 - a^2*x^2 + a^2*y^4 - a^2*y^2 - 3*a*x^2 - 3*a*y^2 + a + 1);
func g = 0;

// Solution
Vh uh,vh;
macro grad(u) [dx(u), dy(u)] //
solve poisson2d(uh,vh,solver=CG)=int2d(Th)(grad(uh)'*grad(vh))
-int2d(Th)(f*vh)
+on(1,uh=g);
plot(uh,fill=1,value=1,cmm="uh",wait=1);

//L2 error (normalized)
real num=int2d(Th)((uexact-uh)*(uexact-uh)); num=sqrt(num);
real den1=int2d(Th)(uexact*uexact); den1=sqrt(den1);
real den2=int2d(Th)(uh*uh); den2=sqrt(den2);
real den=(den1+den2)/2;
cout<<"Relative error in  L2 norm = "<<num/den<<endl;

// Expected output:
/*
  --  mesh:  Nb of Triangles =    236, Nb of Vertices 137
Degrees of freedom N = 137
--  mesh:  Nb of Triangles =   5706, Nb of Vertices 2944
-- Solve : 
min -0.981309  max -1.42832e-31
Relative error in  L2 norm = 0.0165159
times: compile 0.016902s, execution 0.093685s,  mpirank:0
CodeAlloc : nb ptr  2902,  size :366776 mpirank: 0
Ok: Normal End
*/

\end{lstlisting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\begin{thebibliography}{10}
	\addcontentsline{toc}{chapter}{Bibliography}
	
	\bibitem[APSC]{PACS} Advanced Programming for Scientific Computing course lectures and material, Politecnico di Milano, 2019.
	\bibitem[Andrew NG]{AndrewNG} Professor Andrew NG's Deep Learning MOOC at Coursera, \href{https://www.coursera.org/specializations/deep-learning}{\emph{https://www.coursera.org/specializations/deep-learning}}, consulted in 2019.
	\bibitem[Boost]{boost} Boost C++ libraries, \href{https://www.boost.org/}{\emph{https://www.boost.org/}}
	\bibitem[cppreference.com]{cppreference} An online reference of the C++ language, \href{https://en.cppreference.com/w/cpp}{\emph{https://en.cppreference.com/w/cpp}}
	\bibitem[Eigen]{eigen} Eigen C++ template library for linear algebra, \href{https://eigen.tuxfamily.org/}{\emph{https://eigen.tuxfamily.org/}}
	\bibitem[FreeFem++]{freefem++} FreeFem++ PDE solver, \href{https://freefem.org/}{\emph{https://freefem.org/}}
	\bibitem[GetPot]{getpot} GetPot command line parser, \href{http://getpot.sourceforge.net/}{\emph{http://getpot.sourceforge.net/}}
	\bibitem[Git]{git} Git distributed version control system, \href{https://git-scm.com/}{\emph{https://git-scm.com/}}
	\bibitem[gnuplot]{gnuplot} A GNU graphing utility, \href{http://www.gnuplot.info/}{\emph{http://www.gnuplot.info/}}
	\bibitem[gnuplot-iostream]{gnuplot-iostream} A C++ interface to gnuplot, \href{https://github.com/dstahlke/gnuplot-iostream}{\emph{https://github.com/dstahlke/gnuplot-iostream}}
	\bibitem[Jinchao Xu et al.]{Jinchao} Juncai He, Lin li, Jinchao Xu, Chunyue Zheng, \emph{ReLU Deep Neural Networks and Linear Finite Elements}, 2018, found at \href{https://arxiv.org/abs/1807.03973}{\emph{https://arxiv.org/abs/1807.03973}}
	\bibitem[Kailai et al.]{Kailai} Kailai Xu, Bella Shi, Shuyi Yin, code and technical report of the project for the course CS230 \emph{Deep Learning, Winter 2018, Stanford University}, found at \href{https://github.com/kailaix/nnpde}{\emph{https://github.com/kailaix/nnpde}}
	\bibitem[Kingma-LeiBa]{Kingma} Diederik P. Kingma, Jimmy Lei Ba,\emph{Adam: a method for stochastic optimization}, 2015, found at
	\href{https://arxiv.org/abs/1412.6980}{\emph{https://arxiv.org/abs/1412.6980}} 
	\bibitem[Lagaris et al.]{Lagaris} I.E. Lagaris, A. Likas and D.I. Fotiadis: \emph{Artificial Neural Networks for Solving Ordinary and Partial Differential Equations}, 1997, found at \href{https://arxiv.org/abs/physics/9705023}{\emph{https://arxiv.org/abs/physics/9705023}}
	\bibitem[Make]{make} GNU Make doc at \href{https://www.gnu.org/software/make/manual}{\emph{https://www.gnu.org/software/make/manual}}
	\bibitem[Sirignano-Spiliopoulos]{Sirignano}Justin Sirignano, Konstantinos Spiliopoulos: \emph{DGM: A deep learning algorithm for solving partial differential equations}, 2018, found at \href{https://arxiv.org/abs/1708.07469}{\emph{https://arxiv.org/abs/1708.07469}}
	\bibitem[Weinan-Bing]{Weinan} Weinan E, Bing Yu \emph{The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems}, 2017, found at \href{https://arxiv.org/abs/1710.00211}{\emph{https://arxiv.org/abs/1710.00211}}
	
\end{thebibliography}


\end{document}	
